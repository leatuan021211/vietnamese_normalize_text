{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# I. Normalize Text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Nomalize ancent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "khuyên1 nghi5 it5\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import json\n",
    "\n",
    "# Replacement function \n",
    "def normalize_accented_word(text):\n",
    "    pattern = re.compile(r'(\\w*)([áàảãạấầẩẫậắằẳẵặéèẻẽẹếềểễệíìỉĩịóòỏõọốồổỗộớờởỡợúùủũụứừửữựýỳỷỹỵ])(\\w*)')\n",
    "    with open('bank/tones.json', 'r') as file: \n",
    "        tone_map = json.load(file)\n",
    "    def replace(match):\n",
    "        accented_char = match.group(2)\n",
    "        base_char, tone = tone_map[accented_char]\n",
    "        return f\"{match.group(1)}{base_char}{match.group(3)}{tone}\"\n",
    "    \n",
    "    text = pattern.sub(replace, text)\n",
    "    return text\n",
    "\n",
    "# Example usage\n",
    "text = \"khuyến nghị ịt\"\n",
    "modified_text = normalize_accented_word(text)\n",
    "print(modified_text)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normalize Number"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "with open(\"bank/base_numbers.json\", \"r\") as file:\n",
    "    BASE_NUMBERS = {int(key): value for key, value in json.load(file).items()}\n",
    "\n",
    "with open(\"bank/number_levels.json\", \"r\") as file:\n",
    "    NUMBER_LEVELS = {int(key): value for key, value in json.load(file).items()}\n",
    "\n",
    "\n",
    "def convert_number_2_digits(num: int):\n",
    "    if num in BASE_NUMBERS:\n",
    "        return BASE_NUMBERS[num]\n",
    "    \n",
    "    tens = num // 10\n",
    "    base = num % 10\n",
    "    if base > 0:\n",
    "        return f\"{BASE_NUMBERS[tens]} mươi {BASE_NUMBERS[base]}\"\n",
    "    \n",
    "    return f\"{BASE_NUMBERS[tens]} mươi\"\n",
    "\n",
    "\n",
    "def convert_number_3_digits(num:int):\n",
    "    if num == 0:\n",
    "        return \"\"\n",
    "\n",
    "    remainder = num % 100\n",
    "    hundred = num // 100\n",
    "    if remainder == 0:\n",
    "        return f\"{BASE_NUMBERS[hundred]} trăm\"\n",
    "    \n",
    "    if remainder < 10:\n",
    "        return f\"{BASE_NUMBERS[num // 100]} trăm linh {BASE_NUMBERS[remainder]}\"\n",
    "    \n",
    "    return f\"{BASE_NUMBERS[hundred]} trăm {convert_number_2_digits(remainder)}\"\n",
    "\n",
    "\n",
    "def convert_number_string_to_vietnamese(num:int):\n",
    "    if num == 0:\n",
    "        return 'không'\n",
    "\n",
    "    if num in BASE_NUMBERS:\n",
    "        return BASE_NUMBERS[num]\n",
    "    \n",
    "    if num < 100:\n",
    "        return convert_number_2_digits(num)\n",
    "    \n",
    "    result = convert_number_3_digits(num % 1000)\n",
    "    current_level = None\n",
    "    \n",
    "    for current_level in NUMBER_LEVELS:\n",
    "        next_level = current_level * 1000\n",
    "        if num // (next_level) == 0:\n",
    "            break\n",
    "\n",
    "        result = convert_number_3_digits(num % (next_level) // current_level) + \" \" + NUMBER_LEVELS[current_level] + \" \" + result\n",
    "        \n",
    "    level_base = num // current_level    \n",
    "    \n",
    "    if level_base == 0:\n",
    "        return result\n",
    "    \n",
    "    if level_base in BASE_NUMBERS:\n",
    "        return  f'{BASE_NUMBERS[level_base]} {NUMBER_LEVELS[current_level]} {result}'\n",
    "    \n",
    "    if level_base  > 99:\n",
    "        return f'{convert_number_3_digits(level_base)} {NUMBER_LEVELS[current_level]} {result}'\n",
    "    \n",
    "    if level_base > 11:\n",
    "        return  f'{convert_number_2_digits(level_base)} {NUMBER_LEVELS[current_level]} {result}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def normalize_number(text: str) -> str:\n",
    "    # Regular expression to match numbers\n",
    "    pattern = r'\\d+'\n",
    "    \n",
    "    # Use re.sub with a lambda to replace each number with its Vietnamese word equivalent\n",
    "    replaced_text = re.sub(pattern, lambda x: convert_number_string_to_vietnamese(int(x.group())), text)\n",
    "    \n",
    "    return replaced_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'một trăm linh ba, một trăm hai mươi ba, một trăm mười ba, hai trăm hai mươi mốt, một nghìn không trăm linh chín, một nghìn một trăm linh chín, mười nghìn một trăm linh một'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "normalize_number(\"103, 123, 113, 221, 1009, 1109, 10101\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normalize unit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5 kí lô gam và 10 mét khối nước\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import json\n",
    "\n",
    "# Hàm chuyển đổi đơn vị đo lường về tiếng Việt\n",
    "def normalize_unit(text):\n",
    "    # Từ điển chứa các đơn vị đo lường và bản dịch sang tiếng Việt\n",
    "    with open('bank/units.json', 'r', encoding='utf-8') as json_file:\n",
    "        units_mapping = json.load(json_file)\n",
    "    # Sort units by length in descending order to prevent partial replacements\n",
    "    sorted_units = sorted(units_mapping.keys(), key=len, reverse=True)\n",
    "\n",
    "    # Create a single regex pattern for all units\n",
    "    pattern = r'\\b(' + '|'.join(map(re.escape, sorted_units)) + r')\\b'\n",
    "    \n",
    "    # Replacement function to map the matched unit to its Vietnamese equivalent\n",
    "    def replace_unit(match):\n",
    "        return units_mapping[match.group(0)]\n",
    "    \n",
    "    # Use re.sub with the compiled pattern and replacement function\n",
    "    return re.sub(pattern, replace_unit, text)\n",
    "\n",
    "# Example usage:\n",
    "text = \"5 kg và 10 m3 nước\"\n",
    "converted_text = normalize_unit(text)\n",
    "print(converted_text)  # Output: \"5 kí lô gam và 10 mét khối nước\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normalize Acronym"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'đảng cộng sản việt nam'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "def normalize_acronym(text: str):\n",
    "    with open('bank/acronyms.json', 'r', encoding='utf-8') as json_file:\n",
    "        acronym_map = json.load(json_file)\n",
    "        \n",
    "    sorted_units = sorted(acronym_map.keys(), key=len, reverse=True)\n",
    "    \n",
    "    pattern = r'\\b(' + '|'.join(map(re.escape, sorted_units)) + r')\\b'\n",
    "    \n",
    "    def replace_unit(match):\n",
    "        return acronym_map[match.group(0)]\n",
    "    \n",
    "    return re.sub(pattern, replace_unit, text)\n",
    "\n",
    "text = 'đcsvn'\n",
    "normalize_acronym(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normalize letter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ang gờ a'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "def normalize_letter(text: str):\n",
    "    with open('bank/letters.json', 'r', encoding='utf-8') as json_file:\n",
    "        letter_map = json.load(json_file)\n",
    "        \n",
    "    # Sort units by length in descending order to prevent partial replacements\n",
    "    sorted_units = sorted(letter_map.keys(), key=len, reverse=True)\n",
    "    \n",
    "    # Create a single regex pattern for all units\n",
    "    pattern = r'\\b(' + '|'.join(map(re.escape, sorted_units)) + r')\\b'\n",
    "    \n",
    "    # Replacement function to map the matched unit to its Vietnamese equivalent\n",
    "    def replace_unit(match):\n",
    "        return letter_map[match.group(0)]\n",
    "    \n",
    "    # Use re.sub with the compiled pattern and replacement function\n",
    "    return re.sub(pattern, replace_unit, text)\n",
    "\n",
    "text = 'ang g a'\n",
    "normalize_letter(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normalize symbol"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "with open('bank/symbols.json', 'r', encoding='utf-8') as json_file:\n",
    "    SYMBOLS = dict(sorted(json.load(json_file).items(), key=lambda item: len(item[0]), reverse=True))\n",
    "    \n",
    "def normalize_symbol(text: str):\n",
    "    pattern = r'([\\s\\S])(' + '|'.join(map(re.escape, SYMBOLS)) + r')([\\s\\S])'\n",
    "    \n",
    "    def replace_symbol(match):\n",
    "        return (match.group(1) if match.group(1) == ' ' else match.group(1) + ' ') + SYMBOLS[match.group(2)] + (match.group(3) if match.group(3) == ' ' else match.group(3) + ' ')\n",
    "    \n",
    "    return re.sub(pattern, replace_symbol, text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'a khác b'"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = 'a<> b'\n",
    "normalize_symbol(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normalize dot and comma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def normalize_dot_and_comma(text):\n",
    "    signs = {\n",
    "        '.': ' chấm ',\n",
    "        ',': ' phẩy ',\n",
    "    }\n",
    "    \n",
    "    def replace_dot_and_comma(match):\n",
    "        return match.group(1) + signs[match.group(2)] + match.group(3)\n",
    "    \n",
    "    # remove duplicate\n",
    "    text = re.sub(r'\\.{2,}', '.', text)\n",
    "    text = re.sub(r'\\,{2,}', ',', text)\n",
    "    \n",
    "    text = re.sub(r'(\\S)([,\\.])(\\S)', replace_dot_and_comma, text)\n",
    "    text = re.sub(r'(\\S)([,\\.])', r'\\1 \\2', text)\n",
    "    text = re.sub(r'([,\\.])(\\S)', r'\\1 \\2', text)\n",
    "    return text \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'. . , 1 chấm 2 2 phẩy 3'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "normalize_dot_and_comma('.... . , 1.2 2,3')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normalize Same Phoneme"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ciều không dống gép qan ngệ ngi'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open('bank/same_phonemes.json', 'r', encoding='utf-8') as json_file:\n",
    "    SAME_PHONEMES = dict(sorted(json.load(json_file).items(), key=lambda item: len(item[0]), reverse=True))\n",
    "\n",
    "def normalize_same_phoneme(text: str):\n",
    "    \n",
    "    # Create a single regex pattern for all symbols\n",
    "    pattern = r'(' + '|'.join(map(re.escape, SAME_PHONEMES)) + r')'\n",
    "    \n",
    "    # Replacement function to map the matched symbol to its Vietnamese equivalent\n",
    "    def replace_symbol(match):\n",
    "        return SAME_PHONEMES[match.group(0)]\n",
    "    \n",
    "    # Use re.sub with the compiled pattern and replacement function\n",
    "    return re.sub(pattern, replace_symbol, text)\n",
    "\n",
    "text = 'kiều không giống ghép quan nghệ nghi'\n",
    "normalize_same_phoneme(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normalize Date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('bank/date_prefixs.json', 'r', encoding='utf-8') as json_file:\n",
    "    DATE_PREFIXS = sorted(json.load(json_file), key=len, reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def normalize_date_pattern1(text: str):\n",
    "    # Date pattern 1\n",
    "    date_pattern1 = r'(\\b\\w{0,4}\\b)\\s*([12][0-9]|3[01]|0?[1-9])\\/(1[0-2]|0?[1-9])\\/(\\d{1,4})'  # Example: 11/12/2002\n",
    "\n",
    "    def replace(match):\n",
    "        prefix = match.group(1).strip()\n",
    "        day = match.group(2)\n",
    "        month = match.group(3)\n",
    "        year = match.group(4)\n",
    "        \n",
    "        if prefix == 'ngày':\n",
    "            return f'{prefix} {day} tháng {month} năm {year}'\n",
    "        else:\n",
    "            return f'{prefix + \" \" if prefix != \"\" else \"\"}ngày {day} tháng {month} năm {year}'\n",
    "\n",
    "    return re.sub(date_pattern1, replace, text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ngày 11 tháng 12 năm 2002'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "normalize_date_pattern1('ngày 11/12/2002')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ngày 11 tháng 12 năm 2002'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "normalize_date_pattern1('11/12/2002')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'trưa ngày 11 tháng 12 năm 2002'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "normalize_date_pattern1('trưa 11/12/2002')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def normalize_date_pattern2(text: str):\n",
    "    # Date pattern 1\n",
    "    date_pattern2 = r'(\\b\\w{0,4}\\b)\\s([12][0-9]|3[01]|0?[1-9])\\-(1[0-2]|0?[1-9])\\-(\\d{1,4})'  # Example: 11/12/2002\n",
    "\n",
    "    def replace(match):\n",
    "        prefix = match.group(1).strip()\n",
    "        day = match.group(2)\n",
    "        month = match.group(3)\n",
    "        year = match.group(4)\n",
    "        \n",
    "        if prefix == 'ngày':\n",
    "            return f'{prefix} {day} tháng {month} năm {year}'\n",
    "        else:\n",
    "            return f'{prefix + \" \" if prefix != \"\" else \"\"}ngày {day} tháng {month} năm {year}'\n",
    "\n",
    "    return re.sub(date_pattern2, replace, text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'sáng ngày 11 tháng 12 năm 2002'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "normalize_date_pattern2('sáng 11-12-2002')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'thứ hai ngày 11 tháng 12 năm 2002'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "normalize_date_pattern2('thứ hai 11-12-2002')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def normalize_date_pattern3(text: str):\n",
    "    # Date pattern 3\n",
    "    date_pattern3 = r'(\\b\\w{0,5}\\b)\\s*(0?[1-9]|1[0,1,2])[\\/|\\-](\\d{4})' # Example: 12/2022 -> tháng 12 năm 2002\n",
    "\n",
    "    def replace(match):\n",
    "        prefix = match.group(1).strip()\n",
    "        month = match.group(2)\n",
    "        year = match.group(3)\n",
    "        print(prefix)\n",
    "        if prefix == 'tháng':\n",
    "            return f'{prefix} tháng {month} năm {year}'\n",
    "        else:\n",
    "            return f'{prefix + \" \" if prefix != \"\" else \"\"}tháng {month} năm {year}'\n",
    "\n",
    "    return re.sub(date_pattern3, replace, text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'tháng 12 năm 2002'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "normalize_date_pattern3('12-2002')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tháng\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'tháng tháng 12 năm 2002'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "normalize_date_pattern3('tháng 12-2002')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def normalize_date_pattern4(text: str):\n",
    "    # Date pattern 4\n",
    "    prefixs = \"|\".join(DATE_PREFIXS)\n",
    "    date_pattern4 = r'(' + prefixs + r')\\s([12][0-9]|3[01]|0?[1-9])[\\-|\\/](1[0-2]|0?[1-9])[\\-|\\/](\\d{1,4})'\n",
    "\n",
    "    return re.sub(date_pattern4, r'\\1 ngày \\2 tháng \\3 năm \\4', text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'chiều ngày 11 tháng 12 năm 2002'"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "normalize_date_pattern4('chiều 11/12/2002')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_date(text: str):\n",
    "    text = normalize_date_pattern4(text)\n",
    "    text = normalize_date_pattern1(text)\n",
    "    text = normalize_date_pattern2(text)\n",
    "    text = normalize_date_pattern3(text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normalize all text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "\n",
    "\n",
    "# Function to replace numbers with Vietnamese words and add spaces around commas\n",
    "def normalize_vietnamese_text(text):\n",
    "    # Convert text to lowercase\n",
    "    text = text.lower()\n",
    "    \n",
    "    # Normalize Date\n",
    "    text = normalize_date(text)\n",
    "    \n",
    "    # Replace numbers with words\n",
    "    text = normalize_number(text)\n",
    "    \n",
    "    # Normalize Accent\n",
    "    text = normalize_accented_word(text)\n",
    "    \n",
    "    # Handle punctuation (remove unnecessary symbols, keep meaningful punctuation)\n",
    "    text = re.sub(r'[^a-zA-Z0-9\\sđâăêôơư.,]', '', text)\n",
    "    \n",
    "    # Normalize dot and comma\n",
    "    text = normalize_dot_and_comma(text)\n",
    "    \n",
    "    text = normalize_unit(text)\n",
    "    \n",
    "    text = normalize_same_phoneme(text)\n",
    "    \n",
    "    return [t for t in text.split(' ')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['sang1', 'nay', 'ngay2', 'mươi2', 'môt5', 'thang1', 'mươi2', 'hai', 'năm', 'hai', 'ngin2', 'không', 'trăm', 'linh', 'hai', ',', 'lưc5', '.', 'lương5', 'cưu1', 'hô5', 'đa4', 'mơ3', 'rông5', 'pham5', 'vi', 'tim2', 'ciêm1', 'thêm', 'mươi2', 'môt5', 'triêu5', 'môt5', 'trăm', 'hai', 'mươi', 'hai', 'ngin2', 'không', 'trăm', 'linh', 'hai', 'mét', 'vê2', 'phia1', 'canh1', 'đông2', 'gân2', 'khu', 'vưc5', 'nha2', 'ơ3', 'cua3', 'cac1', 'hô5', 'dân', '.', 'thêm', 'hai', 'nan5', 'nhân', 'vưa2', 'đươc5', 'tim2', 'thây1', ',', 'nâng', 'tông3', 'sô1', 'ngươi2', 'tư3', 'vong', 'lên', 'ba', 'mươi', 'hai', '.']\n"
     ]
    }
   ],
   "source": [
    "text = 'Sáng nay 11/12/2002, lực... lượng cứu hộ đã mở rộng phạm vi tìm kiếm thêm 11122002 m về phía cánh đồng gần khu vực nhà ở của các hộ dân. Thêm hai nạn nhân vừa được tìm thấy, nâng tổng số người tử vong lên 32.'\n",
    "print(normalize_vietnamese_text(text=text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# II. Anayze Phoneme"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "alphabet = [\n",
    "    'a', 'ă', 'â', 'b', 'c', 'd', 'đ', 'e', 'ê', 'g', 'h', 'i', \n",
    "    'k', 'l', 'm', 'n', 'o', 'ô', 'ơ', 'p', 'q', 'r', 's', 't', \n",
    "    'u', 'ư', 'v', 'x', 'y'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dyads which never take a final consonant\n",
    "group_1 = [\n",
    "    'ai', 'ao', 'au', 'ay', 'âu', 'ây', 'eo', 'êu', 'iu', \n",
    "    'ia', 'oi', 'ôi', 'ơi', 'ua', 'ui', 'ưa', 'ưi', 'ưu'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dyads that optional need a final constant\n",
    "group_2= [\n",
    "    'oa', 'oe', 'uê', 'ươ', 'uy' \n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dyads that always need a final constant\n",
    "group_3 = [\n",
    "    'a', 'ă', 'â', 'e', 'ê', 'i', 'o', 'ô', 'ơ', 'u', 'ư', 'iê', 'oă', 'oo', 'uâ', 'uô', 'ươ', 'uyê'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Triads\n",
    "group_4 = [\n",
    "    'iêu', 'oai', 'oao', 'oay', 'oeo', 'uây', 'uôi', 'uya', \n",
    "    'uyu', 'ươi', 'ươu'\n",
    "]\n",
    "# Note: 'uyê' q|uyen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_consonants = [\n",
    "    'c', 'ch', 'ng', 'nh', 'm', 'n', 'p', 't'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "head_consonants = [\n",
    "    'b', 'c', 'd', 'đ', 'g', 'h', 'l', 'm', 'n', 'p', 'q', \n",
    "    'r', 's', 't', 'v', 'x', 'ch', 'kh', 'ng', 'nh', 'ph', 'th', 'tr' \n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "accent_list = ['1', '2', '3', '4', '5']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "phoneme_bank = alphabet + group_1 + group_2+ head_consonants + \\\n",
    "            [char1 + char2 for char1 in group_2 for char2 in final_consonants] + \\\n",
    "            [char1 + char2 for char1 in group_3 for char2 in final_consonants] + accent_list + [',', '.']\n",
    "\n",
    "phoneme_bank.sort(key=len, reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "266"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(phoneme_bank)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['uyêch', 'uyêng', 'uyênh', 'oach', 'oang', 'oanh', 'oech', 'oeng', 'oenh', 'uêch', 'uêng', 'uênh', 'ươch', 'ương', 'ươnh', 'uych', 'uyng', 'uynh', 'iêch', 'iêng', 'iênh', 'oăch', 'oăng', 'oănh', 'ooch', 'oong', 'oonh', 'uâch', 'uâng', 'uânh', 'uôch', 'uông', 'uônh', 'ươch', 'ương', 'ươnh', 'uyêc', 'uyêm', 'uyên', 'uyêp', 'uyêt', 'oac', 'oam', 'oan', 'oap', 'oat', 'oec', 'oem', 'oen', 'oep', 'oet', 'uêc', 'uêm', 'uên', 'uêp', 'uêt', 'ươc', 'ươm', 'ươn', 'ươp', 'ươt', 'uyc', 'uym', 'uyn', 'uyp', 'uyt', 'ach', 'ang', 'anh', 'ăch', 'ăng', 'ănh', 'âch', 'âng', 'ânh', 'ech', 'eng', 'enh', 'êch', 'êng', 'ênh', 'ich', 'ing', 'inh', 'och', 'ong', 'onh', 'ôch', 'ông', 'ônh', 'ơch', 'ơng', 'ơnh', 'uch', 'ung', 'unh', 'ưch', 'ưng', 'ưnh', 'iêc', 'iêm', 'iên', 'iêp', 'iêt', 'oăc', 'oăm', 'oăn', 'oăp', 'oăt', 'ooc', 'oom', 'oon', 'oop', 'oot', 'uâc', 'uâm', 'uân', 'uâp', 'uât', 'uôc', 'uôm', 'uôn', 'uôp', 'uôt', 'ươc', 'ươm', 'ươn', 'ươp', 'ươt', 'ai', 'ao', 'au', 'ay', 'âu', 'ây', 'eo', 'êu', 'iu', 'ia', 'oi', 'ôi', 'ơi', 'ua', 'ui', 'ưa', 'ưi', 'ưu', 'oa', 'oe', 'uê', 'ươ', 'uy', 'ch', 'kh', 'ng', 'nh', 'ph', 'th', 'tr', 'ac', 'am', 'an', 'ap', 'at', 'ăc', 'ăm', 'ăn', 'ăp', 'ăt', 'âc', 'âm', 'ân', 'âp', 'ât', 'ec', 'em', 'en', 'ep', 'et', 'êc', 'êm', 'ên', 'êp', 'êt', 'ic', 'im', 'in', 'ip', 'it', 'oc', 'om', 'on', 'op', 'ot', 'ôc', 'ôm', 'ôn', 'ôp', 'ôt', 'ơc', 'ơm', 'ơn', 'ơp', 'ơt', 'uc', 'um', 'un', 'up', 'ut', 'ưc', 'ưm', 'ưn', 'ưp', 'ưt', 'a', 'ă', 'â', 'b', 'c', 'd', 'đ', 'e', 'ê', 'g', 'h', 'i', 'k', 'l', 'm', 'n', 'o', 'ô', 'ơ', 'p', 'q', 'r', 's', 't', 'u', 'ư', 'v', 'x', 'y', 'b', 'c', 'd', 'đ', 'g', 'h', 'l', 'm', 'n', 'p', 'q', 'r', 's', 't', 'v', 'x', '1', '2', '3', '4', '5', ',', '.']\n"
     ]
    }
   ],
   "source": [
    "print(phoneme_bank)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def word2vec(word:str):\n",
    "    v = [0] * len(phoneme_bank)\n",
    "    for idx in range(0, len(phoneme_bank)):\n",
    "        if word == '':\n",
    "            return v\n",
    "        if phoneme_bank[idx] in word:\n",
    "            v[idx] = 1\n",
    "            # print(f\"Found {phoneme_bank[idx]} in {word}\")\n",
    "            word = re.sub(phoneme_bank[idx], '', word)\n",
    "            # print(word)\n",
    "    return v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'normalize_vietnamese_text' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[66], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m text \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mSáng nay, lực lượng cứu hộ đã mở rộng phạm vi tìm kiếm thêm 1010000101 m về phía cánh đồng gần khu vực nhà ở của các hộ dân. Thêm hai nạn nhân vừa được tìm thấy, nâng tổng số người tử vong lên 32.\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m----> 2\u001b[0m words \u001b[38;5;241m=\u001b[39m \u001b[43mnormalize_vietnamese_text\u001b[49m(text\u001b[38;5;241m=\u001b[39mtext)\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(words)\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m word \u001b[38;5;129;01min\u001b[39;00m words:\n",
      "\u001b[0;31mNameError\u001b[0m: name 'normalize_vietnamese_text' is not defined"
     ]
    }
   ],
   "source": [
    "text = 'Sáng nay, lực lượng cứu hộ đã mở rộng phạm vi tìm kiếm thêm 1010000101 m về phía cánh đồng gần khu vực nhà ở của các hộ dân. Thêm hai nạn nhân vừa được tìm thấy, nâng tổng số người tử vong lên 32.'\n",
    "words = normalize_vietnamese_text(text=text)\n",
    "print(words)\n",
    "for word in words:\n",
    "    print(f\"{word}: {sum(word2vec(word))}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "quyên : 2\n"
     ]
    }
   ],
   "source": [
    "print(f\"quyên : {sum(word2vec('quyên'))}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "with open(\"phonemes.json\", \"w\") as file:\n",
    "    json.dump(phoneme_bank, file, indent='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "266"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(phoneme_bank)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pbl6",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
